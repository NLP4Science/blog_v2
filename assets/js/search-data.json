{
  
    
        "post0": {
            "title": "Title",
            "content": "cd ~/transformers_fork/examples/summarization/ . /Users/shleifer/transformers_fork/examples/summarization . ls . __init__.py bart/ bertabs/ . cd bertabs . /Users/shleifer/transformers_fork/examples/summarization/bertabs . ls . README.md __init__.py configuration_bertabs.py convert_bertabs_original_pytorch_checkpoint.py modeling_bertabs.py requirements.txt run_summarization.py test_utils_summarization.py utils_summarization.py . from modeling_bertabs import * . model = BertAbs.from_pretrained(&quot;bertabs-finetuned-cnndm&quot;) . .",
            "url": "https://sshleifer.github.io/blog_v2/2020/03/10/bert_abs.count.html",
            "relUrl": "/2020/03/10/bert_abs.count.html",
            "date": " • Mar 10, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Introducing BART",
            "content": "Overview . For the past few weeks, I worked on integrating BART into transformers. This post covers the high-level differences between BART and its predecessors, how to use new BartForConditionalGeneration to summarize documents, and a few interesting engineering tricks that accelerate generation. . Background: Seq2Seq Pretraining . In October 2019, teams from Microsoft, Google and Facebook independently published three new transformer papers: UniLM, T5 and BART. All three papers found that they achieve better downstream performance on generation tasks, like translation and summarization, if they (a) replace BERT&#39;s bidirectional architecture with a Seq2Seq architecture and (b) BERT&#39;s fill-in-the blank cloze task with a more complicated mix of pretraining tasks. . Now let&#39;s dig deeper into the big Seq2Seq pretraining idea! . Bert vs. GPT2 . As the Bart authors&#39; write, . (Bart) can be seen as generalizing Bert (due to the bidirectional encoder) and GPT2 (with the left to right decoder). . Bert is pretrained to try to predict masked tokens, and uses the whole sequence to get enough info to make a good guess. This is good for tasks where the prediction at position i is allowed to utilize information from positions after i, but less useful for tasks where you are not, like text generation, where you generate the next word conditional on the previously generated words. . In code, the idea of &quot;what information you can use when predicting the token at position i&quot; is controlled by an argument called attention_mask1 . Here is Bert&#39;s &quot;Fully-visible&quot; attention_mask: the 1 in the top left cell means that the model can use information from &lt;BOS&gt; when it is making predictions about &lt;EOS&gt;. . . . the same parameter that is used to make model predictions invariant to pad tokens.&#8617; . | !open diagram_bartpost_bert.jpg . GPT2, meanwhile, is pretrained to predict the next word using a causal mask: and is more effective for generation tasks, but less effective on downstream tasks where the whole input yields information for the output. . Here is the attention_mask for GPT2: . . Encoder-Decoder . Our new friends, like Bart, get the best of both worlds: the masking style of bert and the left to right generation style of GPT2. . The encoder&#39;s attention_mask is bidirectional: . and the decoder&#39;s attention_mask is causal: . . The encoder and decoder are connected by &quot;cross-attention&quot;: each decoder layer performs attention over the final hidden state of the encoder output. This presumably nudges the models towards generating output that is closely connected to the original input. . Summarization . Now, we apply this Seq2Seq method to summarization, where the input sequence is the document we want to summarize, and the output sequence is a ground truth summary. The Seq2Seq attention pattern is therefore well suited to summarization and other conditional generation tasks. Seq2Seq models are allowed to attend to the whole input document, but, as they generate a summary, they can only consider what they&#39;ve already generated. The numbers confirm this: all the new fancy Seq2Seq models do a lot better than the old less-fancy guys on the CNN/Daily Mail abstractive summarization task, and Bart does especially well. . rouge2 model_size Pretraining . PT-Gen (not seq2seq) | 17.28 | 22 M | None | . TransformerAbs | 17.76 | 200M | None | . BertSumABS | 19.39 | 220 M | Encoder | . UniLM | 20.3 | 340 M | Seq2Seq | . T5-base | 20.34 | 770 M | Seq2Seq | . Bart | 21.28 | 406 M | Seq2Seq | . T5-11B | 21.55 | 11 B | Seq2Seq | . BertSumABS (from Text Summarization with Pretrained Encoders, uses a Seq2Seq architecture but doesn&#39;t pretrain the decoder. TransformerAbs, from the same paper, uses a slightly smaller model and no pretraining. PT-Gen is from Get To The Point: Summarization with Pointer-Generator Networks . BartForConditionalGeneration . imports . #collapse-hide INSTALL_MSG = &quot;&quot;&quot; Bart will be released through pip in v 3.0.0, until then use it by installing from source: git clone git@github.com:huggingface/transformers.git cd transformers pip install -e &quot;.[dev]&quot; &quot;&quot;&quot; import torch import transformers # Installed from source at commit e03129ad try: from transformers import BartTokenizer, BartForConditionalGeneration except ImportError: raise ImportError(INSTALL_MSG) from IPython.display import display, Markdown torch_device = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39; LONG_BORING_TENNIS_ARTICLE = &quot;&quot;&quot; Andy Murray came close to giving himself some extra preparation time for his w edding next week before ensuring that he still has unfinished tennis business to attend to. The world No 4 is into the semi-finals of the Miami Open, but not be fore getting a scare from 21 year-old Austrian Dominic Thiem, who pushed him to 4-4 in the second set before going down 3-6 6-4, 6-1 in an hour and three quarte rs. Murray was awaiting the winner from the last eight match between Tomas Berdy ch and Argentina&#39;s Juan Monaco. Prior to this tournament Thiem lost in the secon d round of a Challenger event to soon-to-be new Brit Aljaz Bedene. Andy Murray p umps his first after defeating Dominic Thiem to reach the Miami Open semi finals . Muray throws his sweatband into the crowd after completing a 3-6, 6-4, 6-1 vi ctory in Florida . Murray shakes hands with Thiem who he described as a &#39;strong guy&#39; after the game . And Murray has a fairly simple message for any of his fell ow British tennis players who might be agitated about his imminent arrival into the home ranks: don&#39;t complain. Instead the British No 1 believes his colleagues should use the assimilation of the world number 83, originally from Slovenia, a s motivation to better themselves. At present any grumbles are happening in priv ate, and Bedene&#39;s present ineligibility for the Davis Cup team has made it less of an issue, although that could change if his appeal to play is allowed by the International Tennis Federation. Murray thinks anyone questioning the move, now it has become official, would be better working on getting their ranking closer to his. &#39;If he was 500 in the world they wouldn&#39;t be that fussed about it but ob viously he threatens their position a bit,&#39; said the 27 year-old Scot. &#39; and he&#39; s obviously the British number two, comfortably. &#39;So they can complain but the b est thing to do is use it in the right way and accept it for what it is, and try to use it as motivation whether they agree with it or not. He&#39;s British now so they&#39;ve just got to deal with it. Murray stretches for a return after starting h is quarter final match slowly on the show court . Thiem held nothing back as he raced through the opening set, winning it 6-3 with a single break . The young Au strian is considered to be one of the hottest prospects on the ATP Tour . &#39;I wou ld hope that all the guys who are below him now like James (Ward) , Kyle (Edmund ) , Liam (Broady) they will use it as motivation. If he becomes eligible for Dav is Cup then those guys are going to have to prove themselves. &#39;It can only be se en as a positive for those guys using it to try to get better. He&#39;s a good playe r but so are James and Kyle and Liam has improved. Aljaz is there, he&#39;s on the t our every week, the other guys aren&#39;t quite there yet.&#39; For the first time Murra y, who has an encyclopaedic knowledge of the top 100, gave his opinion of Bedene : &#39;He&#39;s a good player with a very good serve. He&#39;s a legitimate top 100 player, when he plays Challengers he&#39;s there or thereabouts, when he plays on the main t our he wins matches, it&#39;s not like he turns up and always loses in the first rou nd. Murray&#39;s fiancee was once again watching from the stands shaded by a huge br immed hat . Kim Sears flashes her enormous diamond engagement ring while watchin g her beau on court . &#39;He had a bad injury last year (wrist) but has recovered w ell. I would imagine he would keep moving up the rankings although I don&#39;t know exactly how high he can go. I&#39;ve practised with him a couple of times, I haven&#39;t seen him play loads, but when you serve as well as he does it helps. I would im agine he&#39; s going to be comfortably in the top 70 or 80 in the world for a while .&#39; It is understood the Lawn Tennis Association will give background support to his case regarding the Davis Cup but have made it clear that the onus is on him to lead the way. An official statement said: &#39;To have another player in the men&#39; s top 100 is clearly a positive thing for British tennis and so we very much wel come Aljaz&#39;s change in citizenship.&#39; The last comparable switch came twenty year s ago when Greg Rusedski arrived from Canada. It was by no means universally pop ular but, like Bedene, he pledged that he was in for the long haul and, in fairn ess to him, he proved true to his word. Loising the first set shocked Murray int o life as he raced to a commanding lead in the second . The No 3 seed sent over a few glaring looks towards his team before winning the second set . Murray had to put such matters aside as he tackled the unusually talented Thiem, a delight to watch. Coached by Boris Becker&#39;s veteran mentor Gunter Bresnik, he slightly r esembles Andy Roddick and hits with similar power but more elegance. His single handed backhand is a thing of rare beauty. However, he has had a mediocre season coming into this event and there was little to forewarn of his glorious shotmak ing that seemed to catch Murray unawares early on. The world No 4 looked to have worked him out in the second, but then suffered one of his periopdic mental lap ses and let him back in from 4-1 before closing it out with a break. After break ing him for 3-1 in the decider the Austrian whirlwind burnt itself out. &#39;He&#39;s a strong guy who hits the ball hard and it became a very physical match,&#39; said Mur ray. Murray was presented with a celebratory cake after winning his 500th match in the previous round . &quot;&quot;&quot;.replace(&#39; n&#39;,&#39;&#39;) . . #collapse-show tokenizer = BartTokenizer.from_pretrained(&#39;bart-large-cnn&#39;) model = BartForConditionalGeneration.from_pretrained(&#39;bart-large-cnn&#39;) article_input_ids = tokenizer.batch_encode_plus([LONG_BORING_TENNIS_ARTICLE], return_tensors=&#39;pt&#39;, max_length=1024)[&#39;input_ids&#39;].to(torch_device) summary_ids = model.generate(article_input_ids, num_beams=4, length_penalty=2.0, max_length=140, min_len=55) summary_txt = tokenizer.decode(summary_ids.squeeze(), skip_special_tokens=True) display(Markdown(&#39;&gt; **Summary:**:&#39;+summary_txt)) . . Summary::Andy Murray beat Dominic Thiem 3-6, 6-4, 6-1 in the Miami Open. The world No 4 is into the semi-finals of the tournament in Florida. Murray was awaiting the winner from the last eight match between Tomas Berdych and Argentina&#39;s Juan Monaco. Thiem lost in the second round of a Challenger event to Aljaz Bedene. . GPT2 (which in fairness is not finetuned for summarization) cannot really continue the tennis article sensically. . #collapse-show from transformers import GPT2LMHeadModel, GPT2Tokenizer gpt2_tok = GPT2Tokenizer.from_pretrained(&#39;gpt2&#39;) gpt2_model = GPT2LMHeadModel.from_pretrained(&#39;gpt2&#39;, output_past=True) # truncate to 869 tokens so that we have space to generate another 155 enc = gpt2_tok.encode(LONG_BORING_TENNIS_ARTICLE, max_length=1024-155, return_tensors=&#39;pt&#39;) # Generate another 155 tokens source_and_summary_ids = gpt2_model.generate(enc, max_length=1024, do_sample=False) # Only show the new ones end_of_source = &quot;An official statement said:&quot; _, summary_gpt2 = gpt2_tok.decode(source_and_summary_ids[0]).split(end_of_source) display(Markdown(&#39;&gt; **GPT2:** &#39; + summary_gpt2)) . . Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence . GPT2: &#39;To have a player like James Ward, Kyle Edmund, Liam Broady and Aljaz Bedene in the top 100 is a huge achievement for the Lawn Tennis Association. The Lawn Tennis Association is committed to the development of the sport and the development of the sport&#39;s players. The Lawn Tennis Association is committed to the development of the sport and the development of the sport&#39;s players. The Lawn Tennis Association is committed to the development of the sport and the development of the sport&#39;s players. The Lawn Tennis Association is committed to the development of the sport and the development of the sport&#39;s players. The Lawn Tennis Association is committed to the development of the sport and the development of the sport&#39;s players. The Lawn Tennis Association is committed to the development of the sport and the development of the . More importantly, these snippets show that even though BartForConditionalGeneration is a seq2seq model, while GPT2LMHeadModel is not, they can invoked in similar ways for generation. . Now that we have a high level understanding of Bart, lets finish by zooming in on some engineering tricks that make generation faster. Warning: The following has a heavier engineering focus than the rest of the post . Incremental Decoding . Here is a really slow (pseudocode) way to greedily generate summaries: . output_tokens = [bos] while not done: encoder_hidden_state = model.encoder(article_input_ids) logits = model.decoder(encoder_hidden_state, output_tokens) next_word = logits.argmax() output_tokens.append(next_word) if next_word == eos: break . Let&#39;s say N = len(article_input_ids) and M = len(output_tokens) when we return. Let&#39;s also ignore the cost of encoder-decoder attention. Then the complexity of this approach is roughly . $(N cdot M)$ (we call the encoder M times on N tokens) | $+ sum_{m=1}^{M} m = frac{M(M+1)}{2}$ (we call the decoder M times on 1 token then 2 tokens, all the way to M tokens). | Total: $N cdot M + frac{M(M+1)}{2}$ | . Let&#39;s say we are generating a 100 token summary of a 1024 token article, then we have to &quot;process&quot; $N cdot M + frac{M(M+1)}{2} = 107,450$ tokens . #collapse-hide # show work N,M=1024, 100 complexity_simple = int(N*M + ((M*(M+1))/2)) msg = &quot;To generate a 100 token summary of a 1024 token article using this approach, we have to process {:,} tokens.&quot; #display(Markdown(msg.format(complexity_simple))) . . Thankfully, we can just hoist the encoder call outside the loop! . output_tokens = [bos] encoder_hidden_state = model.encoder(article_input_ids) while not done: logits = model.decoder(encoder_hidden_state, output_tokens) next_word = logits.argmax() output_tokens.append(next_word) if next_word == eos: break . Now the complexity is dramatically reduced: $N cdot 1 + frac{M(M+1)}{2} = 6,074$ . But we can go even further, and partially cache the attention outputs for the decoder. This will change our generation loop to: . output_tokens = [bos] encoder_hidden_state = model.encoder(article_input_ids) cache = None while not done: logits, cache = model.decoder(encoder_hidden_state, output_tokens[-1], cache=cache) next_word = logits.argmax() output_tokens.append(next_word) if next_word == eos: break . And change our formula to $N + M = 1,124$ . The nitty-gritty of this trick is explained in the Bonus section below. . Conclusion . Our first release of BartModel prioritized moving quickly and keeping the code simple, but it&#39;s still a work in progress. I am currently working on making the implementation in transformers as fast and memory-efficient as the original implementation. Stay tuned for episode 2 of this series, where we try to close the gap. . A big thank you to Sasha Rush and Patrick Von Platten for reading many versions of this post, and to the Bart authors for releasing their code and answering questions on Github. . Bonus: Partially caching k and v in DecoderLayer . Here is some pseudocode for attention without all the reshapes and heads and masks and scaling. . class SimplifiedAttention(nn.Module): def __init__(self, embed_dim): self.Wq = torch.nn.Linear(embed_dim, embed_dim) self.Wk = torch.nn.Linear(embed_dim, embed_dim) self.Wv = torch.nn.Linear(embed_dim, embed_dim) self.dense = torch.nn.Linear(embed_dim, embed_dim) def forward(self, query, key, value): q = self.Wq(q) k = self.Wk(k) v = self.Wv(v) matmul_qk = torch.matmul(q, k.T) attention_weights = matmul_qk.softmax(dim=-1) output = torch.matmul(attention_weights, v) return self.dense(output) . Now lets glimpse at the callers inside bart&#39;s DecoderLayer: (LayerNorms and dropouts deleted for simplicity). Here&#39;s some more pseudocode . class SimplifiedDecoderLayer(nn.Module): def __init__(self, embed_dim): self.self_attn = SimplifiedAttention(embed_dim) self.encoder_attn = SimplifiedAttention(embed_dim) def forward(x, last_encoder_hidden_state, *masks_etc): # x shape `(batch_size, tokens_generated_so_far, embed_dim)` # x comes from decoder x = self.self_attn(query=x, key=x, value=x) # pay attention to somebody else for a change! output = self.encoder_attn( query=x, key=last_encoder_hidden_state, # could be None value=last_encoder_hidden_state, ) return output . What did we learn? . In encoder_attention, we can cache everything that doesn&#39;t depend on q, namely these outputs k = self.Wk(k) v = self.Wv(v) . | . The more exciting optimization is that in self_attn, we can cache the part of k,v that depends on x[:, :1] the tokens we&#39;ve already generated. Then each time through the generation loop, we only pass in x[:, :-1] and apply concatenation: . k = torch.cat((past_key, new_k), dim=&#39;seq_len&#39;) # the seq_len dimension, v = torch.cat((past_value, new_v), dim=&#39;seq_len&#39;) . Of the 8 F.linear ops performed by each DecoderLayer was doing, we&#39;ve managed to completely cache 2 of them, and almost completely cache 2 more. Overall, we chop off about 40% of the runtime vs only caching encoder_outputs. .",
            "url": "https://sshleifer.github.io/blog_v2/jupyter/2020/03/06/bart.html",
            "relUrl": "/jupyter/2020/03/06/bart.html",
            "date": " • Mar 6, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . Front Matter is a markdown cell at the beginning of your notebook that allows you to inject metadata into your notebook. For example: . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks just like you can with markdown. . For example, here is a footnote 1. . . This is the footnote.&#8617; . |",
            "url": "https://sshleifer.github.io/blog_v2/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Test Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://sshleifer.github.io/blog_v2/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://sshleifer.github.io/blog_v2/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}