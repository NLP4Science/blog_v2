<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Introducing BART | TensorGoose</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Introducing BART" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Episode 1 – a mysterious new Seq2Seq model with state of the art summarization performance visits a popular open source library" />
<meta property="og:description" content="Episode 1 – a mysterious new Seq2Seq model with state of the art summarization performance visits a popular open source library" />
<link rel="canonical" href="https://sshleifer.github.io/blog_v2/jupyter/2020/03/12/bart.html" />
<meta property="og:url" content="https://sshleifer.github.io/blog_v2/jupyter/2020/03/12/bart.html" />
<meta property="og:site_name" content="TensorGoose" />
<meta property="og:image" content="https://sshleifer.github.io/blog_v2/images/text_infilling.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-03-12T00:00:00-05:00" />
<script type="application/ld+json">
{"datePublished":"2020-03-12T00:00:00-05:00","headline":"Introducing BART","image":"https://sshleifer.github.io/blog_v2/images/text_infilling.png","description":"Episode 1 – a mysterious new Seq2Seq model with state of the art summarization performance visits a popular open source library","mainEntityOfPage":{"@type":"WebPage","@id":"https://sshleifer.github.io/blog_v2/jupyter/2020/03/12/bart.html"},"@type":"BlogPosting","url":"https://sshleifer.github.io/blog_v2/jupyter/2020/03/12/bart.html","dateModified":"2020-03-12T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog_v2/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://sshleifer.github.io/blog_v2/feed.xml" title="TensorGoose" /><script async src="https://www.googletagmanager.com/gtag/js?id=UA-160410879-1"></script>
<script>
  window['ga-disable-UA-160410879-1'] = window.doNotTrack === "1" || navigator.doNotTrack === "1" || navigator.doNotTrack === "yes" || navigator.msDoNotTrack === "1";
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-160410879-1');
</script>
<link rel="shortcut icon" type="image/x-icon" href="/blog_v2/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Introducing BART | TensorGoose</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Introducing BART" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Episode 1 – a mysterious new Seq2Seq model with state of the art summarization performance visits a popular open source library" />
<meta property="og:description" content="Episode 1 – a mysterious new Seq2Seq model with state of the art summarization performance visits a popular open source library" />
<link rel="canonical" href="https://sshleifer.github.io/blog_v2/jupyter/2020/03/12/bart.html" />
<meta property="og:url" content="https://sshleifer.github.io/blog_v2/jupyter/2020/03/12/bart.html" />
<meta property="og:site_name" content="TensorGoose" />
<meta property="og:image" content="https://sshleifer.github.io/blog_v2/images/text_infilling.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-03-12T00:00:00-05:00" />
<script type="application/ld+json">
{"datePublished":"2020-03-12T00:00:00-05:00","headline":"Introducing BART","image":"https://sshleifer.github.io/blog_v2/images/text_infilling.png","description":"Episode 1 – a mysterious new Seq2Seq model with state of the art summarization performance visits a popular open source library","mainEntityOfPage":{"@type":"WebPage","@id":"https://sshleifer.github.io/blog_v2/jupyter/2020/03/12/bart.html"},"@type":"BlogPosting","url":"https://sshleifer.github.io/blog_v2/jupyter/2020/03/12/bart.html","dateModified":"2020-03-12T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://sshleifer.github.io/blog_v2/feed.xml" title="TensorGoose" /><script async src="https://www.googletagmanager.com/gtag/js?id=UA-160410879-1"></script>
<script>
  window['ga-disable-UA-160410879-1'] = window.doNotTrack === "1" || navigator.doNotTrack === "1" || navigator.doNotTrack === "yes" || navigator.msDoNotTrack === "1";
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-160410879-1');
</script>

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    // remove paragraph tags in rendered toc (happens from notebooks)
    var toctags = document.querySelectorAll(".toc-entry")
    toctags.forEach(e => (e.firstElementChild.innerText = e.firstElementChild.innerText.replace('¶', '')))
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog_v2/">TensorGoose</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog_v2/about/">About Me</a><a class="page-link" href="/blog_v2/search/">Search</a><a class="page-link" href="/blog_v2/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Introducing BART</h1><p class="page-description">Episode 1 -- a mysterious new Seq2Seq model with state of the art summarization performance visits a popular open source library</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-03-12T00:00:00-05:00" itemprop="datePublished">
        Mar 12, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      12 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog_v2/categories/#jupyter">jupyter</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/sshleifer/blog_v2/tree/master/_notebooks/2020-03-12-bart.ipynb" role="button">
<img class="notebook-badge-image" src="/blog_v2/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div><div class="px-2">
    <a href="https://colab.research.google.com/github/sshleifer/blog_v2/blob/master/_notebooks/2020-03-12-bart.ipynb">
        <img class="notebook-badge-image" src="/blog_v2/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h3"><a href="#Overview">Overview </a></li>
<li class="toc-entry toc-h3"><a href="#Background:-Seq2Seq-Pretraining">Background: Seq2Seq Pretraining </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Bert-vs.-GPT2">Bert vs. GPT2 </a></li>
<li class="toc-entry toc-h4"><a href="#Encoder-Decoder">Encoder-Decoder </a></li>
<li class="toc-entry toc-h4"><a href="#Pretraining:-Fill-In-the-Span">Pretraining: Fill In the Span </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#Summarization">Summarization </a></li>
<li class="toc-entry toc-h3"><a href="#Demo:-BartForConditionalGeneration">Demo: BartForConditionalGeneration </a></li>
<li class="toc-entry toc-h2"><a href="#Conclusion">Conclusion </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-03-12-bart.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Overview">
<a class="anchor" href="#Overview" aria-hidden="true"><span class="octicon octicon-link"></span></a>Overview<a class="anchor-link" href="#Overview"> </a>
</h3>
<p>For the past few weeks, I worked on integrating BART into <a href="https://github.com/huggingface/transformers/">transformers</a>. This post covers the high-level differences between BART and its predecessors and how to use the new <code>BartForConditionalGeneration</code> to summarize documents. Leave a comment below if you have any questions!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Background:-Seq2Seq-Pretraining">
<a class="anchor" href="#Background:-Seq2Seq-Pretraining" aria-hidden="true"><span class="octicon octicon-link"></span></a>Background: Seq2Seq Pretraining<a class="anchor-link" href="#Background:-Seq2Seq-Pretraining"> </a>
</h3>
<p>In October 2019, teams from Google and Facebook published new transformer papers:  <a href="https://arxiv.org/abs/1910.10683">T5</a> and <a href="https://arxiv.org/abs/1910.13461">BART</a>. Both papers achieved better downstream performance on generation tasks, like abstractive summarization and dialogue, with two changes:</p>
<ul>
<li>add a BERT's bidirectional encoder architecture with an encoder-decoder architecture</li>
<li>replace BERT's fill-in-the blank cloze task with a more complicated mix of pretraining tasks.</li>
</ul>
<!-- **Tasks:** Historically, Seq2Seq models have been used for text generation tasks like summarization and translation. "BART is particularly effective when finetuned for text generation, but also matches the performance of RoBERTa on GLUE and SQuAD", with only 10% more parameters. -->
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now let's dig deeper into the big Seq2Seq pretraining idea!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Bert-vs.-GPT2">
<a class="anchor" href="#Bert-vs.-GPT2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Bert vs. GPT2<a class="anchor-link" href="#Bert-vs.-GPT2"> </a>
</h4>
<p>As the BART authors write,</p>
<blockquote>
<p>(BART) can be seen as generalizing Bert (due to the bidirectional encoder) and GPT2 (with the left to right decoder).</p>
</blockquote>
<p>Bert is pretrained to try to predict masked tokens, and uses the whole sequence to get enough info to make a good guess. This is good for tasks where the prediction at position <code>i</code> is allowed to utilize information from positions after <code>i</code>, but less useful for tasks, like text generation, where the prediction for position <code>i</code> can only depend on previously generated words.</p>
<p>In code, the idea of "what information can be used use when predicting the token at position <code>i</code>" is controlled by an argument called <code>attention_mask</code><sup class="footnote-ref" id="fnref-2"><a href="#fn-2">1</a></sup>. A value of 1 in the attention mask means that the model can use information for the column's word when predicting the row's word.</p>
<!-- > Note: In this post, we show attention masks in grids where each row `y` represents an output token, and each column `x` represents an input token. If the square at `(y3, x4)` is black. It means that our prediction for `y3` is allowed to utilize information from `x4`. During pretraining, `x` would be the corrupted document, and `y` would be the original. -->

<p>Here is Bert's "Fully-visible"<sup class="footnote-ref" id="fnref-3"><a href="#fn-3">2</a></sup> <code>attention_mask</code>:</p>
<p><!-- ![](./bert_mac_small.jpg) -->
<!-- ![](./diagram_bartpost_v2.jpg) -->
<!-- ![](./bert_excel_v2.jpg) -->
<img src="/blog_v2/images/copied_from_nb/./diagram_bert_v5.png" alt=""></p>
<div class="footnotes">
<hr>
<ol>
<li id="fn-2"><p>the same parameter that is used to make model predictions invariant to pad tokens.<a href="#fnref-2" class="footnote">↩</a></p></li>
<li id="fn-3"><p>"Fully-Visible" and "bidirectional" are used interchangeably. Same with "causal" and "autoregressive".<a href="#fnref-3" class="footnote">↩</a></p></li>
</ol>
</div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>GPT2, meanwhile, is pretrained to predict the next word using a causal mask, and is more effective for generation tasks, but less effective on downstream tasks where the whole input yields information for the output.</p>
<p>Here is the <code>attention_mask</code> for GPT2:</p>
<p><img src="/blog_v2/images/copied_from_nb/./diagram_bartpost_gpt2.jpg" alt=""></p>
<p>The prediction for "eating", only utilizes previous words: "<code>&lt;BOS&gt;</code> I love".</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Encoder-Decoder">
<a class="anchor" href="#Encoder-Decoder" aria-hidden="true"><span class="octicon octicon-link"></span></a>Encoder-Decoder<a class="anchor-link" href="#Encoder-Decoder"> </a>
</h4>
<p>Our new friends, like BART, get the best of both worlds.</p>
<p>The encoder's <code>attention_mask</code> is fully visible, like BERT:
<img src="/blog_v2/images/copied_from_nb/./seq2seq_enc_v5.png" alt=""></p>
<p>The decoder's <code>attention_mask</code> is causal, like GPT2:</p>
<p><img src="/blog_v2/images/copied_from_nb/./seq2seq_dec.png" alt=""></p>
<!-- ![](./causal_with_prefix.jpg) -->


<!-- We can think about this `attention_mask` as smushing together our previous two attention masks, or "Causal Mask  with a fully visible prefix" in fancier terms.[^4] -->

<!-- [^4]: The UniLM paper presents this as a"causal mask with a fully visible prefix" -->
<!-- , as the UniLM The indices dont line up perfectly for the smush to work, but tokens 1 and 2 are the fully visible prefix (or the input to the encoder) and tokens 3,4,5 are the causally masked suffix (or inputs to the decoder). In summarization terms, you could imagine tokens 1 and 2 as the article, and we generate tokens 3-5 auto-regressively. -->

<!-- ![](./t5_mask_diagram.png) -->
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The encoder and decoder are connected by cross-attention, where each decoder layer performs attention over the final hidden state of the encoder output. This presumably nudges the models towards generating output that is closely connected to the original input.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Pretraining:-Fill-In-the-Span">
<a class="anchor" href="#Pretraining:-Fill-In-the-Span" aria-hidden="true"><span class="octicon octicon-link"></span></a>Pretraining: Fill In the Span<a class="anchor-link" href="#Pretraining:-Fill-In-the-Span"> </a>
</h4>
<p>Bart and T5 are both pretrained<sup class="footnote-ref" id="fnref-5"><a href="#fn-5">1</a></sup> on tasks where <strong>spans</strong> of text are replaced by masked tokens. The model must learn to reconstruct the original document. Figure 1 from the BART paper explains it well:</p>
<p><img src="/blog_v2/images/copied_from_nb/./text_infilling.png" alt="">
In this example, the original document is A B C D E. the span <code>[C, D]</code> is masked before encoding and an extra mask is inserted before B, leaving the corrupted document <code>'A _ B _ E'</code> as input to the encoder.</p>
<p>The decoder (autogressive means "uses a causal mask") must reconstruct the original document, using the encoder's output and previous uncorrupted tokens.</p>
<div class="footnotes">
<hr>
<ol>
<li id="fn-5"><p>This is a bit of a simplification. Both papers experiment with many different pretraining tasks, and find that this one performs well. T5 uses a "replace corrupted spans" task. Instead of putting masks, they put in a random token.<a href="#fnref-5" class="footnote">↩</a></p></li>
</ol>
</div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Summarization">
<a class="anchor" href="#Summarization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Summarization<a class="anchor-link" href="#Summarization"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In summarization tasks, the <code>input</code> sequence is the document we want to summarize, and the <code>output</code> sequence is a ground truth summary.
Seq2Seq archictectures can be directly finetuned on summarization tasks, without any new randomly initialized heads. The pretraining task is also a good match for the downstream task. In both settings, the input document must be copied from the input with modification. The numbers confirm this: all the new fancy Seq2Seq models do a lot better than the old less-fancy guys on the CNN/Daily Mail abstractive summarization task, and BART does especially well.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<table>
<thead>
<tr>
<th style="text-align:left">Model</th>
<th style="text-align:right">Rouge2</th>
<th style="text-align:left">Model Size</th>
<th style="text-align:left">Pretraining</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">PT-Gen</td>
<td style="text-align:right">17.28</td>
<td style="text-align:left">22 M</td>
<td style="text-align:left">None</td>
</tr>
<tr>
<td style="text-align:left">TransformerAbs</td>
<td style="text-align:right">17.76</td>
<td style="text-align:left">200M</td>
<td style="text-align:left">None</td>
</tr>
<tr>
<td style="text-align:left">BertSumABS</td>
<td style="text-align:right">19.39</td>
<td style="text-align:left">220 M</td>
<td style="text-align:left">Encoder</td>
</tr>
<tr>
<td style="text-align:left">UniLM</td>
<td style="text-align:right">20.3</td>
<td style="text-align:left">340 M</td>
<td style="text-align:left">Seq2Seq</td>
</tr>
<tr>
<td style="text-align:left">T5-base</td>
<td style="text-align:right">20.34</td>
<td style="text-align:left">770 M</td>
<td style="text-align:left">Seq2Seq</td>
</tr>
<tr>
<td style="text-align:left">Bart</td>
<td style="text-align:right">21.28</td>
<td style="text-align:left">406 M</td>
<td style="text-align:left">Seq2Seq</td>
</tr>
<tr>
<td style="text-align:left">T5-11B</td>
<td style="text-align:right">21.55</td>
<td style="text-align:left">11 B</td>
<td style="text-align:left">Seq2Seq</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>
<code>BertSumABS</code> (from <a href="https://arxiv.org/abs/1908.08345"><em>Text Summarization with Pretrained Encoders</em></a>, uses a Seq2Seq architecture but doesn't pretrain the decoder. <code>TransformerAbs</code>, from the same paper, uses a slightly smaller model and no pretraining. </li>
<li>
<code>PT-Gen</code> is from <a href="https://arxiv.org/pdf/1704.04368.pdf">Get To The Point: Summarization with Pointer-Generator Networks</a>
</li>
<li>
<a href="https://arxiv.org/abs/1905.03197">UniLM</a> is a "Prefix-LM" with a similar masking strategy to Bart and T5.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Demo:-BartForConditionalGeneration">
<a class="anchor" href="#Demo:-BartForConditionalGeneration" aria-hidden="true"><span class="octicon octicon-link"></span></a>Demo: BartForConditionalGeneration<a class="anchor-link" href="#Demo:-BartForConditionalGeneration"> </a>
</h3>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#collapse-hide</span>
<span class="n">INSTALL_MSG</span> <span class="o">=</span> <span class="s2">"""</span>
<span class="s2">Bart will be released through pip in v 3.0.0, until then use it by installing from source:</span>

<span class="s2">git clone git@github.com:huggingface/transformers.git</span>
<span class="s2">cd transformers</span>
<span class="s2">pip install -e ".[dev]"</span>

<span class="s2">"""</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">transformers</span>  <span class="c1">#  Installed from source at commit e03129ad</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BartTokenizer</span><span class="p">,</span> <span class="n">BartForConditionalGeneration</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="n">INSTALL_MSG</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span><span class="p">,</span> <span class="n">Markdown</span>

<span class="n">torch_device</span> <span class="o">=</span> <span class="s1">'cuda'</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">'cpu'</span>

<span class="n">LONG_BORING_TENNIS_ARTICLE</span> <span class="o">=</span> <span class="s2">"""</span>
<span class="s2"> Andy Murray  came close to giving himself some extra preparation time for his w</span>
<span class="s2">edding next week before ensuring that he still has unfinished tennis business to</span>
<span class="s2"> attend to. The world No 4 is into the semi-finals of the Miami Open, but not be</span>
<span class="s2">fore getting a scare from 21 year-old Austrian Dominic Thiem, who pushed him to </span>
<span class="s2">4-4 in the second set before going down 3-6 6-4, 6-1 in an hour and three quarte</span>
<span class="s2">rs. Murray was awaiting the winner from the last eight match between Tomas Berdy</span>
<span class="s2">ch and Argentina's Juan Monaco. Prior to this tournament Thiem lost in the secon</span>
<span class="s2">d round of a Challenger event to soon-to-be new Brit Aljaz Bedene. Andy Murray p</span>
<span class="s2">umps his first after defeating Dominic Thiem to reach the Miami Open semi finals</span>
<span class="s2"> . Muray throws his sweatband into the crowd after completing a 3-6, 6-4, 6-1 vi</span>
<span class="s2">ctory in Florida . Murray shakes hands with Thiem who he described as a 'strong </span>
<span class="s2">guy' after the game . And Murray has a fairly simple message for any of his fell</span>
<span class="s2">ow British tennis players who might be agitated about his imminent arrival into </span>
<span class="s2">the home ranks: don't complain. Instead the British No 1 believes his colleagues</span>
<span class="s2"> should use the assimilation of the world number 83, originally from Slovenia, a</span>
<span class="s2">s motivation to better themselves. At present any grumbles are happening in priv</span>
<span class="s2">ate, and Bedene's present ineligibility for the Davis Cup team has made it less </span>
<span class="s2">of an issue, although that could change if his appeal to play is allowed by the </span>
<span class="s2">International Tennis Federation. Murray thinks anyone questioning the move, now </span>
<span class="s2">it has become official, would be better working on getting their ranking closer </span>
<span class="s2">to his. 'If he was 500 in the world they wouldn't be that fussed about it but ob</span>
<span class="s2">viously he threatens their position a bit,' said the 27 year-old Scot. ' and he'</span>
<span class="s2">s obviously the British number two, comfortably. 'So they can complain but the b</span>
<span class="s2">est thing to do is use it in the right way and accept it for what it is, and try</span>
<span class="s2"> to use it as motivation whether they agree with it or not. He's British now so </span>
<span class="s2">they've just got to deal with it. Murray stretches for a return after starting h</span>
<span class="s2">is quarter final match slowly on the show court . Thiem held nothing back as he </span>
<span class="s2">raced through the opening set, winning it 6-3 with a single break . The young Au</span>
<span class="s2">strian is considered to be one of the hottest prospects on the ATP Tour . 'I wou</span>
<span class="s2">ld hope that all the guys who are below him now like James (Ward) , Kyle (Edmund</span>
<span class="s2">) , Liam (Broady) they will use it as motivation. If he becomes eligible for Dav</span>
<span class="s2">is Cup then those guys are going to have to prove themselves. 'It can only be se</span>
<span class="s2">en as a positive for those guys using it to try to get better. He's a good playe</span>
<span class="s2">r but so are James and Kyle and Liam has improved. Aljaz is there, he's on the t</span>
<span class="s2">our every week, the other guys aren't quite there yet.' For the first time Murra</span>
<span class="s2">y, who has an encyclopaedic knowledge of the top 100, gave his opinion of Bedene</span>
<span class="s2">: 'He's a good player with a very good serve. He's a legitimate top 100 player, </span>
<span class="s2">when he plays Challengers he's there or thereabouts, when he plays on the main t</span>
<span class="s2">our he wins matches, it's not like he turns up and always loses in the first rou</span>
<span class="s2">nd. Murray's fiancee was once again watching from the stands shaded by a huge br</span>
<span class="s2">immed hat . Kim Sears flashes her enormous diamond engagement ring while watchin</span>
<span class="s2">g her beau on court . 'He had a bad injury last year (wrist) but has recovered w</span>
<span class="s2">ell. I would imagine he would keep moving up the rankings although I don't know </span>
<span class="s2">exactly how high he can go. I've practised with him a couple of times, I haven't</span>
<span class="s2"> seen him play loads, but when you serve as well as he does it helps. I would im</span>
<span class="s2">agine he' s going to be comfortably in the top 70 or 80 in the world for a while</span>
<span class="s2">.' It is understood the Lawn Tennis Association will give background support to </span>
<span class="s2">his case regarding the Davis Cup but have made it clear that the onus is on him </span>
<span class="s2">to lead the way. An official statement said: 'To have another player in the men'</span>
<span class="s2">s top 100 is clearly a positive thing for British tennis and so we very much wel</span>
<span class="s2">come Aljaz's change in citizenship.' The last comparable switch came twenty year</span>
<span class="s2">s ago when Greg Rusedski arrived from Canada. It was by no means universally pop</span>
<span class="s2">ular but, like Bedene, he pledged that he was in for the long haul and, in fairn</span>
<span class="s2">ess to him, he proved true to his word. Loising the first set shocked Murray int</span>
<span class="s2">o life as he raced to a commanding lead in the second . The No 3 seed sent over </span>
<span class="s2">a few glaring looks towards his team before winning the second set . Murray had </span>
<span class="s2">to put such matters aside as he tackled the unusually talented Thiem, a delight </span>
<span class="s2">to watch. Coached by Boris Becker's veteran mentor Gunter Bresnik, he slightly r</span>
<span class="s2">esembles Andy Roddick and hits with similar power but more elegance. His single </span>
<span class="s2">handed backhand is a thing of rare beauty. However, he has had a mediocre season</span>
<span class="s2"> coming into this event and there was little to forewarn of his glorious shotmak</span>
<span class="s2">ing that seemed to catch Murray unawares early on. The world No 4 looked to have</span>
<span class="s2"> worked him out in the second, but then suffered one of his periopdic mental lap</span>
<span class="s2">ses and let him back in from 4-1 before closing it out with a break. After break</span>
<span class="s2">ing him for 3-1 in the decider the Austrian whirlwind burnt itself out. 'He's a </span>
<span class="s2">strong guy who hits the ball hard and it became a very physical match,' said Mur</span>
<span class="s2">ray. Murray was presented with a celebratory cake after winning his 500th match </span>
<span class="s2">in the previous round .</span>
<span class="s2">"""</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">,</span><span class="s1">''</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description" open="">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#collapse-show</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BartTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bart-large-cnn'</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BartForConditionalGeneration</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bart-large-cnn'</span><span class="p">)</span>

<span class="n">article_input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_encode_plus</span><span class="p">([</span><span class="n">LONG_BORING_TENNIS_ARTICLE</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">'pt'</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">1024</span><span class="p">)[</span><span class="s1">'input_ids'</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch_device</span><span class="p">)</span>
<span class="n">summary_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">article_input_ids</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">length_penalty</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span>
                             <span class="n">max_length</span><span class="o">=</span><span class="mi">140</span><span class="p">,</span> <span class="n">min_len</span><span class="o">=</span><span class="mi">55</span><span class="p">)</span>

<span class="n">summary_txt</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">summary_ids</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">Markdown</span><span class="p">(</span><span class="s1">'&gt; **Summary: **'</span><span class="o">+</span><span class="n">summary_txt</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<blockquote>
<p><strong>Summary: </strong>Andy Murray beat Dominic Thiem 3-6, 6-4, 6-1 in the Miami Open. The world No 4 is into the semi-finals of the tournament in Florida. Murray was awaiting the winner from the last eight match between Tomas Berdych and Argentina's Juan Monaco. Thiem lost in the second round of a Challenger event to Aljaz Bedene.</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>GPT2, which in fairness is not finetuned for summarization, cannot really continue the tennis article sensically.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description" open="">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#collapse-show</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2LMHeadModel</span><span class="p">,</span> <span class="n">GPT2Tokenizer</span>
<span class="n">gpt2_tok</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'gpt2'</span><span class="p">)</span>
<span class="n">gpt2_model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'gpt2'</span><span class="p">,</span> <span class="n">output_past</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># truncate to 869 tokens so that we have space to generate another 155</span>
<span class="n">enc</span> <span class="o">=</span> <span class="n">gpt2_tok</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">LONG_BORING_TENNIS_ARTICLE</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">1024</span><span class="o">-</span><span class="mi">155</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">'pt'</span><span class="p">)</span> 
<span class="c1"># Generate another 155 tokens</span>
<span class="n">source_and_summary_ids</span> <span class="o">=</span> <span class="n">gpt2_model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">enc</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="c1"># Only show the new ones</span>
<span class="n">end_of_source</span> <span class="o">=</span> <span class="s2">"An official statement said:"</span> 
<span class="n">_</span><span class="p">,</span> <span class="n">summary_gpt2</span> <span class="o">=</span> <span class="n">gpt2_tok</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">source_and_summary_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">end_of_source</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">Markdown</span><span class="p">(</span><span class="s1">'&gt; **GPT2:** '</span> <span class="o">+</span> <span class="n">summary_gpt2</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<blockquote>
<p><strong>GPT2:</strong>  'To have a player like James Ward, Kyle Edmund, Liam Broady and Aljaz Bedene in the top 100 is a huge achievement for the Lawn Tennis Association. The Lawn Tennis Association is committed to the development of the sport and the development of the sport's players. The Lawn Tennis Association is committed to the development of the sport and the development of the sport's players. The Lawn Tennis Association is committed to the development of the sport and the development of the sport's players. The Lawn Tennis Association is committed to the development of the sport and the development of the sport's players. The Lawn Tennis Association is committed to the development of the sport and the development of the sport's players. The Lawn Tennis Association is committed to the development of the sport and the development of the</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>More importantly, these snippets show that even though <code>BartForConditionalGeneration</code> is a Seq2Seq model, while <code>GPT2LMHeadModel</code> is not, they can be invoked in similar ways for generation.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Conclusion">
<a class="anchor" href="#Conclusion" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusion<a class="anchor-link" href="#Conclusion"> </a>
</h2>
<p>Our first release of <code>BartModel</code> prioritized moving quickly and keeping the code simple, but it's still a work in progress. I am currently working on making the implementation in transformers faster and more memory efficient, so stay tuned for episode 2!</p>
<p>A big thank you to Sasha Rush, Patrick Von Platten, Thomas Wolf, Clement Delangue, Victor Sanh, Yacine Jernite, Harrison Chase and Colin Raffel for their feedback on earlier versions of this post, and to the BART authors for releasing their code and answering questions on GitHub.</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="sshleifer/blog_v2"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog_v2/jupyter/2020/03/12/bart.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog_v2/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog_v2/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog_v2/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p></p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/sshleifer" title="sshleifer"><svg class="svg-icon grey"><use xlink:href="/blog_v2/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/sam_shleifer" title="sam_shleifer"><svg class="svg-icon grey"><use xlink:href="/blog_v2/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
