{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bart WalkThrough\n",
    "> A journey through the forward pass\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter]\n",
    "- image: images/chart-preview.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq Pretraining\n",
    "In October 2019, teams from Microsoft, Google and Facebook independently published three new transformer papers: UniLM, T5 and Bart. All three papers analyze found that they achieve better downstream performance on generation tasks if they (a) replace Bert's bidirectional architecture with a seq2seq architecture and (b) Bert's fill-in-the blank cloze task with a more complicated mix of pretraining tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T21:17:48.798208Z",
     "start_time": "2020-03-07T21:17:48.793226Z"
    }
   },
   "source": [
    "Now lets dig deeper into the big Seq2Seq pretraining idea, then dive into some interesting parts of the `BartForConditionalGeneration` code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background: Bert vs. GPT2\n",
    "As the Bart authors' write, \n",
    "> (Bart) can be seen as generalizing Bert (due to the bidirectional encoder) and GPT (with the left to right decoder). \n",
    "\n",
    "\n",
    "Bert is pretrained to try to predict masked tokens, and uses the whole sequence to get enough info to make a good guess. This is good for tasks where the prediction at position `i` is allowed to utilize information from positions after `i`, but less useful for tasks where you are not, like text generation, where you generate the next word conditional on the previously generated words.\n",
    "\n",
    "In code, the idea of \"what information you can use when predicting the token at position `i`\" is controlled by a parameter called `attention_mask`[^2]. \n",
    "> Note: In this post, we show attention masks in grids where each row `y` represents an output token, and each column `x` represents an input token. If the square at `(y3, x4)` is black. It means that our prediction for `y3` is allowed to utilize information from `x4`. During pretraining, `x` would be the corrupted document, and `y` would be the original.\n",
    "\n",
    "\n",
    "Bert's \"Fully-visible\" `attention_mask` is boring:\n",
    "![](./bert_mac_small.jpg)\n",
    "\n",
    "\n",
    "[^2]: the same parameter that is used to make model predictions invariant to pad tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T21:44:38.925844Z",
     "start_time": "2020-03-07T21:44:38.920749Z"
    }
   },
   "source": [
    "GPT2, meanwhile, is pretrained to predict the next word.\n",
    "This makes it good for producing generations, where there aren't future tokens to consider, but less useful for other downstream tasks where the whole sentence is utilized.\n",
    "\n",
    "Here is the `attention_mask` for GPT2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T04:04:22.367554Z",
     "start_time": "2020-03-08T04:04:22.197664Z"
    }
   },
   "source": [
    "![](./gpt2_simple_mask.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder\n",
    "Our new friends get the best of both worlds!\n",
    "\n",
    "The encoder is bidirectional  - each token's attention can attend to every other token in the input sequence, while the decoder, which will ultimately have to perform generation, is causal like GPT2.\n",
    "\n",
    "![](./causal_with_prefix.jpg)\n",
    "\n",
    "\n",
    "We can think about this `attention_mask` as smushing together our previous two attention masks, or \"Causal Mask  with a fully visible prefix\" in fancier terms.[^4]\n",
    "\n",
    "[^4]: The indices dont line up perfectly for the smush to work, but tokens 1 and 2 are the fully visible prefix (or the input to the encoder) and tokens 3,4,5 are the causally masked suffix (or inputs to the decoder). In summarization terms, you could imagine tokens 1 and 2 as the article, and we generate tokens 3-5 auto-regressively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T00:07:18.020243Z",
     "start_time": "2020-03-09T00:07:17.858170Z"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T21:26:57.308704Z",
     "start_time": "2020-03-07T21:26:57.156642Z"
    }
   },
   "source": [
    "\n",
    "![](./t5_mask_diagram.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This attention pattern is very well suited to summarization and other conditional generation tasks.  You are allowed to attend to the whole document, but as you write your summary, one word at a time, you need only consider what you've already written. The numbers confirm this: all the new fancy guys do a lot better than the old less-fancy guys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T21:09:05.584452Z",
     "start_time": "2020-03-07T21:09:05.560477Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CNNDM Rouge 2 score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Paper</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bart</th>\n",
       "      <td>21.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UniLM</th>\n",
       "      <td>20.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BertSumABS</th>\n",
       "      <td>19.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5-base</th>\n",
       "      <td>20.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5 11B</th>\n",
       "      <td>21.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TransformerAbs (2018)</th>\n",
       "      <td>17.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        CNNDM Rouge 2 score \n",
       "Paper                                       \n",
       "Bart                                   21.28\n",
       "UniLM                                  20.30\n",
       "BertSumABS                             19.39\n",
       "t5-base                                20.34\n",
       "t5 11B                                 21.55\n",
       "TransformerAbs (2018)                  17.76"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-hide \n",
    "import pandas as pd\n",
    "pd.read_csv('tab1.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T20:22:23.597072Z",
     "start_time": "2020-03-07T20:22:23.524178Z"
    }
   },
   "source": [
    "<!-- I start with this not to say that the researchers are lame, but that they were all discovering a very cool idea at the same time. In the following table, you can see that, at least for summarization, it is important to pretrain with a bidirectional encoder and causal decoder: (The UniLM `prefix_lm` is equivalent [^3]) -->\n",
    "\n",
    "`BertSumABS` [^2] , exploits the Seq2Seq architecture but doesn't pretrain the decoder.\n",
    "Also note that t5-11b is 22x bigger than Bart), and pretraining objectives.\n",
    "\n",
    "Bart tries out crazy pretraining tasks that you can only do with a seq2seq architecture. Since \"Inputs to the encoder need not be aligned with decoder outputs, allowing arbitary noise transformations.\" They invent a pretraining task called Text Infilling, where you\n",
    "replace a **span** of text with a single mask token. This span can be of any length, so the model also must learn how many tokens to generate.\n",
    "\n",
    "There is also another trick in Bart: each decoder layer performs cross-attention over the final hidden state of the encoder output. This presumably nudges Bart towards generating summaries that are closely connected to the original (encoded) text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T22:30:44.859890Z",
     "start_time": "2020-03-07T22:30:44.839517Z"
    }
   },
   "source": [
    "#### Awkward Transition to Eng\n",
    "\n",
    "Shortly after these papers were released our `transformers` users started asking for us to make them available in the repo, especially Bart. And now, a few months later, it's demo time!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo of  `transformers.BartForConditionalGeneration`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T19:47:09.444089Z",
     "start_time": "2020-03-09T19:47:09.429528Z"
    }
   },
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "def print_80_per_line(txt, n=80):\n",
    "    result = []\n",
    "    for i in range(0, len(txt), n):\n",
    "        result.append(txt[i:i + n])\n",
    "    return '\\n'.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T19:56:01.011023Z",
     "start_time": "2020-03-09T19:56:01.007206Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#collapse-show\n",
    "LONG_BORING_TENNIS_ARTICLE = \"\"\"\n",
    " Andy Murray  came close to giving himself some extra preparation time for his w\n",
    "edding next week before ensuring that he still has unfinished tennis business to\n",
    " attend to. The world No 4 is into the semi-finals of the Miami Open, but not be\n",
    "fore getting a scare from 21 year-old Austrian Dominic Thiem, who pushed him to \n",
    "4-4 in the second set before going down 3-6 6-4, 6-1 in an hour and three quarte\n",
    "rs. Murray was awaiting the winner from the last eight match between Tomas Berdy\n",
    "ch and Argentina's Juan Monaco. Prior to this tournament Thiem lost in the secon\n",
    "d round of a Challenger event to soon-to-be new Brit Aljaz Bedene. Andy Murray p\n",
    "umps his first after defeating Dominic Thiem to reach the Miami Open semi finals\n",
    " . Muray throws his sweatband into the crowd after completing a 3-6, 6-4, 6-1 vi\n",
    "ctory in Florida . Murray shakes hands with Thiem who he described as a 'strong \n",
    "guy' after the game . And Murray has a fairly simple message for any of his fell\n",
    "ow British tennis players who might be agitated about his imminent arrival into \n",
    "the home ranks: don't complain. Instead the British No 1 believes his colleagues\n",
    " should use the assimilation of the world number 83, originally from Slovenia, a\n",
    "s motivation to better themselves. At present any grumbles are happening in priv\n",
    "ate, and Bedene's present ineligibility for the Davis Cup team has made it less \n",
    "of an issue, although that could change if his appeal to play is allowed by the \n",
    "International Tennis Federation. Murray thinks anyone questioning the move, now \n",
    "it has become official, would be better working on getting their ranking closer \n",
    "to his. 'If he was 500 in the world they wouldn't be that fussed about it but ob\n",
    "viously he threatens their position a bit,' said the 27 year-old Scot. ' and he'\n",
    "s obviously the British number two, comfortably. 'So they can complain but the b\n",
    "est thing to do is use it in the right way and accept it for what it is, and try\n",
    " to use it as motivation whether they agree with it or not. He's British now so \n",
    "they've just got to deal with it. Murray stretches for a return after starting h\n",
    "is quarter final match slowly on the show court . Thiem held nothing back as he \n",
    "raced through the opening set, winning it 6-3 with a single break . The young Au\n",
    "strian is considered to be one of the hottest prospects on the ATP Tour . 'I wou\n",
    "ld hope that all the guys who are below him now like James (Ward) , Kyle (Edmund\n",
    ") , Liam (Broady) they will use it as motivation. If he becomes eligible for Dav\n",
    "is Cup then those guys are going to have to prove themselves. 'It can only be se\n",
    "en as a positive for those guys using it to try to get better. He's a good playe\n",
    "r but so are James and Kyle and Liam has improved. Aljaz is there, he's on the t\n",
    "our every week, the other guys aren't quite there yet.' For the first time Murra\n",
    "y, who has an encyclopaedic knowledge of the top 100, gave his opinion of Bedene\n",
    ": 'He's a good player with a very good serve. He's a legitimate top 100 player, \n",
    "when he plays Challengers he's there or thereabouts, when he plays on the main t\n",
    "our he wins matches, it's not like he turns up and always loses in the first rou\n",
    "nd. Murray's fiancee was once again watching from the stands shaded by a huge br\n",
    "immed hat . Kim Sears flashes her enormous diamond engagement ring while watchin\n",
    "g her beau on court . 'He had a bad injury last year (wrist) but has recovered w\n",
    "ell. I would imagine he would keep moving up the rankings although I don't know \n",
    "exactly how high he can go. I've practised with him a couple of times, I haven't\n",
    " seen him play loads, but when you serve as well as he does it helps. I would im\n",
    "agine he' s going to be comfortably in the top 70 or 80 in the world for a while\n",
    ".' It is understood the Lawn Tennis Association will give background support to \n",
    "his case regarding the Davis Cup but have made it clear that the onus is on him \n",
    "to lead the way. An official statement said: 'To have another player in the men'\n",
    "s top 100 is clearly a positive thing for British tennis and so we very much wel\n",
    "come Aljaz's change in citizenship.' The last comparable switch came twenty year\n",
    "s ago when Greg Rusedski arrived from Canada. It was by no means universally pop\n",
    "ular but, like Bedene, he pledged that he was in for the long haul and, in fairn\n",
    "ess to him, he proved true to his word. Loising the first set shocked Murray int\n",
    "o life as he raced to a commanding lead in the second . The No 3 seed sent over \n",
    "a few glaring looks towards his team before winning the second set . Murray had \n",
    "to put such matters aside as he tackled the unusually talented Thiem, a delight \n",
    "to watch. Coached by Boris Becker's veteran mentor Gunter Bresnik, he slightly r\n",
    "esembles Andy Roddick and hits with similar power but more elegance. His single \n",
    "handed backhand is a thing of rare beauty. However, he has had a mediocre season\n",
    " coming into this event and there was little to forewarn of his glorious shotmak\n",
    "ing that seemed to catch Murray unawares early on. The world No 4 looked to have\n",
    " worked him out in the second, but then suffered one of his periopdic mental lap\n",
    "ses and let him back in from 4-1 before closing it out with a break. After break\n",
    "ing him for 3-1 in the decider the Austrian whirlwind burnt itself out. 'He's a \n",
    "strong guy who hits the ball hard and it became a very physical match,' said Mur\n",
    "ray. Murray was presented with a celebratory cake after winning his 500th match \n",
    "in the previous round .\n",
    "\"\"\".replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('bart-large-cnn')\n",
    "model = BartForConditionalGeneration.from_pretrained('bart-large-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T19:56:34.585249Z",
     "start_time": "2020-03-09T19:56:10.469753Z"
    }
   },
   "outputs": [],
   "source": [
    "article_input_ids = tokenizer.batch_encode_plus([LONG_BORING_TENNIS_ARTICLE], return_tensors='pt', max_length=1024)['input_ids'].to(torch_device)\n",
    "summary_ids = model.generate(article_input_ids, num_beams=4, length_penalty=2.0,\n",
    "                             max_length=140, min_len=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T19:56:39.184419Z",
     "start_time": "2020-03-09T19:56:39.176971Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> Andy Murray beat Dominic Thiem 3-6, 6-4, 6-1 in the Miami Open. The world No 4 is into the semi-finals of the tournament in Florida. Murray was awaiting the winner from the last eight match between Tomas Berdych and Argentina's Juan Monaco. Thiem lost in the second round of a Challenger event to Aljaz Bedene."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary_txt = tokenizer.decode(summary_ids.squeeze(), skip_special_tokens=True)\n",
    "display(Markdown('> '+summary_txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T22:48:05.433596Z",
     "start_time": "2020-03-07T22:48:01.245715Z"
    }
   },
   "source": [
    "**TODO**: get conditional generation working on GPT2 for this Doc. The following code just generates `eos`.\n",
    "```python\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "gpt2_tok = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "article_input_ids = gpt2_tok.batch_encode_plus([LONG_ARTICLE], return_tensors='pt', pad_to_max_length=False)['input_ids'].to(torch_device)\n",
    "summary_ids = gpt2_model.generate(article_input_ids, max_length=article_input_ids.shape[1] + 155, do_sample=False)\n",
    "gpt2_tok.decode(summary_ids.squeeze(), ).split('\\n')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to notice in these two snippets: even though `BartForConditionalGeneration` is a seq2seq model, and `GPT2LMHeadModel` is not, they can invoked in similar ways for generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: The same correspondence exists between `BartForSequenceClassification`  and all the other `*ForSequenceClassification` in `transformers`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though you can just pass `input_ids`, like the other models, `BartModel` (and all it's children)'s full signature is a little more complex:\n",
    "```python\n",
    "def forward(\n",
    "    self,\n",
    "    input_ids,\n",
    "    attention_mask=None, # ignored pad tokens in input_ids\n",
    "    decoder_input_ids=None, # make these if not supplied\n",
    "    decoder_padding_mask=None, # ignored pad tokens in decoder_input_ids\n",
    "    decoder_causal_mask=None, # Ignore future tokens in decoder_input_ids\n",
    "):\n",
    "```\n",
    "When we're doing summarization finetuning, or seq2seq pretraining, we need to pass `decoder_input_ids`. (the masks will be made for you if you dont supply them).\n",
    "\n",
    "When we're not, like in a classification context, you can safely ignore all the decoder kwargs and `BartModel` will make them for you by taking the input_ids (movie review) and shifting them to the right. Random, I know.\n",
    "\n",
    "The authors' [motivation](https://github.com/pytorch/fairseq/issues/1389) for the shift-right trick was to facilitate teacher forcing during pre-training, and now that the model has been trained on 64 TPUs for 12 weeks to process this input format, we continue the pattern during inference, but hide it inside the forward method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T04:03:37.743603Z",
     "start_time": "2020-03-07T04:03:37.737338Z"
    }
   },
   "source": [
    "When I first read the fairseq code, there was a function called `make_generation_fast` which didnt do much besides catch my eye. What an exciting name! \n",
    "Anyways, here is a really slow (pseudocode) way to greedily generate summaries\n",
    "    \n",
    "```python\n",
    "output_tokens = []\n",
    "while not done:\n",
    "     encoder_hidden_state = model.encoder(article_input_ids)\n",
    "     logits = model.decoder(encoder_hidden_state, output_tokens)\n",
    "     next_word = logits.argmax()\n",
    "     output_tokens.append(next_word)\n",
    "     if next_word == eos: break\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T03:58:26.921905Z",
     "start_time": "2020-03-07T03:58:26.898646Z"
    }
   },
   "source": [
    "We can just cache the first step and save half the compute\n",
    "\n",
    "```python\n",
    "output_tokens = []\n",
    "encoder_hidden_state = model.encoder(article_input_ids)\n",
    "while not done:\n",
    "     logits = model.decoder(encoder_hidden_state, output_tokens)\n",
    "     next_word = logits.argmax()\n",
    "     \n",
    "     output_tokens.append(next_word)\n",
    "     if next_word == eos: break\n",
    "```\n",
    "Easy peasy, sorry for wasting your time. Here comes the fun one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partially caching k and v in `DecoderLayer`\n",
    "\n",
    "\n",
    "\n",
    "Here is some pseudocode for attention without all the reshapes and heads and masks and scaling.\n",
    "\n",
    "\n",
    "```python\n",
    "class SimplifiedAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        self.Wq = torch.nn.Linear(embed_dim, embed_dim)\n",
    "        self.Wk = torch.nn.Linear(embed_dim, embed_dim)\n",
    "        self.Wv = torch.nn.Linear(embed_dim, embed_dim)\n",
    "        self.dense = torch.nn.Linear(embed_dim, embed_dim)\n",
    "    def forward(self, query, key, value):\n",
    "        q = self.Wq(q)\n",
    "        k = self.Wk(k) \n",
    "        v = self.Wv(v)\n",
    "        matmul_qk = torch.matmul(q, k.T)\n",
    "        attention_weights = matmul_qk.softmax(dim=-1)\n",
    "        output = torch.matmul(attention_weights, v)\n",
    "        return self.dense(output)\n",
    "```\n",
    "\n",
    "Now lets glimpse at the callers inside bart's `DecoderLayer`:\n",
    "(LayerNorms and dropouts deleted for simplicity). Here's some more pseudocode\n",
    "\n",
    "```python\n",
    "class SimplifiedDecoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim):\n",
    "        self.self_attn = SimplifiedAttention(embed_dim)\n",
    "        self.encoder_attn = SimplifiedAttention(embed_dim)\n",
    "    def forward(x, last_encoder_hidden_state, *masks_etc):\n",
    "         # x shape `(batch_size, tokens_generated_so_far, embed_dim)`\n",
    "         # x comes from decoder\n",
    "\n",
    "        x = self.self_attn(query=x, key=x, value=x) # pay attention to somebody else for a change!\n",
    "        output = self.encoder_attn(\n",
    "            query=x,\n",
    "            key=last_encoder_hidden_state,  # could be None\n",
    "            value=last_encoder_hidden_state,\n",
    "        )\n",
    "        return output\n",
    "```\n",
    "\n",
    "What did we learn?\n",
    "\n",
    "\n",
    "- In `encoder_attention`, we can cache everything that doesn't depend on q, namely these outputs\n",
    "```\n",
    "        k = self.Wk(k) \n",
    "        v = self.Wv(v)\n",
    "```\n",
    "\n",
    "\n",
    "The more exciting optimization is that in `self_attn`, we can cache the part of k,v that depends on \n",
    "`x[:, :1]` the tokens we've already generated. Then each time through the generation loop, we only pass in `x[:, :-1]` and apply concatenation:\n",
    "\n",
    "```python\n",
    "k = torch.cat((past_key, new_k), dim='seq_len') # the seq_len dimension, \n",
    "v = torch.cat((past_value, new_v), dim='seq_len')\n",
    "```\n",
    "\n",
    "`TODO(SS): Why cant we cache part of q?`\n",
    "\n",
    "\n",
    "Of the 8 `F.linear` ops performed by each DecoderLayer was doing, we've managed to completely cache 2 of them, and almost completely cache 2 more. Overall, we chop off about 40% of the runtime. `TODO(SS): verify`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Our first release of `BartModel` prioritized moving quickly and keeping the code simple. As a result, our implementation is about 30\\% slower and uses more memory than the authors'. Stay tuned for episode 2 of this series, where we try to close the gap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T21:12:43.347020Z",
     "start_time": "2020-03-07T21:12:43.342222Z"
    }
   },
   "source": [
    "## Footnotes\n",
    "\n",
    "[^2]: \"Text Summarization with Pretrained Encoders\" https://arxiv.org/abs/1908.08345\n",
    "\n",
    "[^3]: Differences between the UniLM Masking strategy and Bart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note \n",
    "Most of our other models do not make inputs for the user -- that's the tokenizer's job, but as the t5 authors write:\n",
    "\n",
    "> \"A major factor that differentiates the architectures is the mask used by different attention mechanisms in the model.\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T22:28:30.773519Z",
     "start_time": "2020-03-08T22:28:30.703222Z"
    }
   },
   "source": [
    "It's a fun game to try to match which of the following quotes from the abstract map to which paper:\n",
    "\n",
    "\n",
    "1. \n",
    "> While many modern approaches to transfer learning for NLP use a Transformer architecture consisting of only a single “stack” (e.g. for language modeling [GPT2]  or classification and span prediction [BERT]), we found that using a standard encoder-decoder structure achieved good results on both generative and classification tasks. \n",
    "\n",
    "2. \n",
    "> The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction.\n",
    "\n",
    "3. \n",
    "> We present a denoising autoencoder for pretraining sequence to sequence models, ... it uses a standard Transformer-based neural machine translation architecture.\n",
    "\n",
    "Answers: [^1]\n",
    "\n",
    " [^1]: Answers: (T5, Oct 24) , (UniLM, Oct. 15) , (Bart, Oct. 29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
