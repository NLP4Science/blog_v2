{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Incremental Decoding\n",
    "\n",
    "> Walks through some of the performance hacks in `BartForConditionalGeneration`\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter]\n",
    "- image: images/chart-preview.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T04:03:37.743603Z",
     "start_time": "2020-03-07T04:03:37.737338Z"
    }
   },
   "source": [
    "Here is a really slow (pseudocode) way to greedily generate summaries:\n",
    "    \n",
    "```python\n",
    "output_tokens = [bos]\n",
    "while not done:\n",
    "     encoder_hidden_state = model.encoder(article_input_ids)\n",
    "     logits = model.decoder(encoder_hidden_state, output_tokens)\n",
    "     next_word = logits.argmax()\n",
    "     output_tokens.append(next_word)\n",
    "     if next_word == eos: break\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say `N = len(article_input_ids)` and `M = len(output_tokens)` when we return. Let's also ignore the cost of encoder-decoder attention.\n",
    "Then the complexity of this approach is roughly\n",
    "- $(N \\cdot M)$ (we call the encoder `M` times on `N` tokens)\n",
    "-  $+ \\sum_{m=1}^{M} m = \\frac{M(M+1)}{2}$   (we call the decoder `M` times on 1 token then 2 tokens, all the way to M tokens).\n",
    "- Total: $N \\cdot M  + \\frac{M(M+1)}{2}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T19:33:40.649588Z",
     "start_time": "2020-03-10T19:33:40.544607Z"
    }
   },
   "source": [
    "Let's say we are generating a 100 token summary of a 1024 token article, then we have to \"process\"\n",
    "$N \\cdot M  + \\frac{M(M+1)}{2} =  107,450$ tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T23:43:48.726602Z",
     "start_time": "2020-03-10T23:43:48.721402Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "# show work\n",
    "N,M=1024, 100\n",
    "complexity_simple = int(N*M + ((M*(M+1))/2))\n",
    "msg = \"To generate a 100 token summary of a 1024 token article using this approach, we have to process {:,} tokens.\"\n",
    "#display(Markdown(msg.format(complexity_simple)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T03:58:26.921905Z",
     "start_time": "2020-03-07T03:58:26.898646Z"
    }
   },
   "source": [
    "Thankfully, we can just hoist the `encoder` call outside the loop!\n",
    "\n",
    "```python\n",
    "output_tokens = [bos]\n",
    "encoder_hidden_state = model.encoder(article_input_ids)\n",
    "while not done:\n",
    "     logits = model.decoder(encoder_hidden_state, output_tokens)\n",
    "     next_word = logits.argmax()\n",
    "     \n",
    "     output_tokens.append(next_word)\n",
    "     if next_word == eos: break\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T19:38:18.003081Z",
     "start_time": "2020-03-10T19:38:17.998496Z"
    }
   },
   "source": [
    "Now the complexity is dramatically reduced: $N \\cdot 1  + \\frac{M(M+1)}{2} = 6,074$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T19:44:12.723917Z",
     "start_time": "2020-03-10T19:44:12.716320Z"
    }
   },
   "source": [
    "But we can  go even further, and partially cache the attention outputs for the decoder.\n",
    "This will change our generation loop to:\n",
    "```python\n",
    "output_tokens = [bos]\n",
    "encoder_hidden_state = model.encoder(article_input_ids)\n",
    "cache = None\n",
    "while not done:\n",
    "     logits, cache = model.decoder(encoder_hidden_state, output_tokens[-1], cache=cache)\n",
    "     next_word = logits.argmax()\n",
    "     output_tokens.append(next_word)\n",
    "     if next_word == eos: break\n",
    "```\n",
    "And change our formula to $N + M = 1,124$\n",
    "\n",
    "The nitty-gritty of this trick is explained in the Bonus section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partially caching keys and values in DecoderLayer\n",
    "\n",
    "\n",
    "\n",
    "Here is some pseudocode for attention without all the reshapes and heads and masks and scaling. It doesn't work even though it looks pretty.\n",
    "\n",
    "\n",
    "```python\n",
    "class SimplifiedAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        self.Wq = torch.nn.Linear(embed_dim, embed_dim)\n",
    "        self.Wk = torch.nn.Linear(embed_dim, embed_dim)\n",
    "        self.Wv = torch.nn.Linear(embed_dim, embed_dim)\n",
    "        self.dense = torch.nn.Linear(embed_dim, embed_dim)\n",
    "    def forward(self, query, key, value):\n",
    "        q = self.Wq(q)\n",
    "        k = self.Wk(k) \n",
    "        v = self.Wv(v)\n",
    "        matmul_qk = torch.matmul(q, k.T)\n",
    "        attention_weights = matmul_qk.softmax(dim=-1)\n",
    "        output = torch.matmul(attention_weights, v)\n",
    "        return self.dense(output)\n",
    "```\n",
    "\n",
    "Now let's glimpse at the callers inside BART's `DecoderLayer`:\n",
    "(LayerNorms and dropouts deleted for simplicity). Here's some more pseudocode\n",
    "\n",
    "```python\n",
    "class SimplifiedDecoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim):\n",
    "        self.self_attn = SimplifiedAttention(embed_dim)\n",
    "        self.encoder_attn = SimplifiedAttention(embed_dim)\n",
    "    def forward(x, last_encoder_hidden_state, *masks_etc):\n",
    "         # x shape `(batch_size, tokens_generated_so_far, embed_dim)`\n",
    "         # x comes from decoder\n",
    "\n",
    "        x = self.self_attn(query=x, key=x, value=x) # pay attention to somebody else for a change!\n",
    "        output = self.encoder_attn(\n",
    "            query=x,\n",
    "            key=last_encoder_hidden_state,  # could be None\n",
    "            value=last_encoder_hidden_state,\n",
    "        )\n",
    "        return output\n",
    "```\n",
    "\n",
    "What did we learn?\n",
    "\n",
    "\n",
    "- In `encoder_attention`, we can cache everything that doesn't depend on q, namely these outputs\n",
    "```\n",
    "        k = self.Wk(k) \n",
    "        v = self.Wv(v)\n",
    "```\n",
    "\n",
    "\n",
    "The more exciting optimization is that in `self_attn`, we can cache the part of k,v that depends on \n",
    "`x[:, :1]` the tokens we've already generated. Then each time through the generation loop, we only pass in `x[:, :-1]` and apply concatenation:\n",
    "\n",
    "```python\n",
    "k = torch.cat((past_key, new_k), dim='seq_len') # the seq_len dimension, \n",
    "v = torch.cat((past_value, new_v), dim='seq_len')\n",
    "```\n",
    "\n",
    "\n",
    "Of the 8 `F.linear` ops performed by each `DecoderLayer`, we've managed to completely cache 2 of them, and almost completely cache 2 more. Overall, we chop off about 40% of the runtime vs only caching encoder_outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
